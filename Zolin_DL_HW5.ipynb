{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxlb0206bjyd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset_builder\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds_builder = load_dataset_builder(\"conll2000\")\n",
        "dataset = load_dataset(\"conll2000\", split=\"train\")\n",
        "\n",
        "pos_numbers = {'0': \"''''\",\n",
        "            '1': \"#\",\n",
        "            '2': \"$\",\n",
        "            '3': \"(\",\n",
        "            '4': \")\",\n",
        "            '5': \",\",\n",
        "            '6': \".\",\n",
        "            '7': \":\",\n",
        "            '8': \"``\",\n",
        "            '9': \"CC\",\n",
        "            '10':\"CD\",\n",
        "            '11':\"DT\",\n",
        "            '12': \"EX\",\n",
        "            '13': \"FW\",\n",
        "            '14': \"IN\",\n",
        "            '15': \"JJ\",\n",
        "            '16': \"JJR\",\n",
        "            '17': \"JJS\",\n",
        "            '18': \"MD\",\n",
        "            '19': \"NN\",\n",
        "            '20': \"NNP\",\n",
        "            '21': \"NNPS\",\n",
        "            '22': \"NNS\",\n",
        "            '23': \"PDT\",\n",
        "            '24': \"POS\",\n",
        "            '25': \"PRP\",\n",
        "            '26': \"PRP$\",\n",
        "            '27': \"RB\",\n",
        "            '28': \"RBR\",\n",
        "            '29': \"RBS\",\n",
        "            '30': \"RP\",\n",
        "            '31': \"SYM\",\n",
        "            '32': \"TO\",\n",
        "            '33': \"UH\",\n",
        "            '34': \"VB\",\n",
        "            '35': \"VBD\",\n",
        "            '36': \"VBG\",\n",
        "            '37': \"VBN\",\n",
        "            '38': \"VBP\",\n",
        "            '39': \"VBZ\",\n",
        "            '40': \"WDT\",\n",
        "            '41': \"WP\",\n",
        "            '42': \"WP$\",\n",
        "            '43': \"WRB\"}\n",
        "\n",
        "\n",
        "sentences = dataset['tokens'][0:2000]\n",
        "tags = dataset['pos_tags'][0:2000]\n",
        "\n",
        "\n",
        "def merge(list1, list2):\n",
        "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
        "    return merged_list\n",
        "\n",
        "training_data = merge(sentences, tags)\n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "num_words = len(word_to_ix)\n",
        "num_tags = len(pos_numbers)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100\n",
        "\n",
        "class LSTMTagger(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(7872, EMBEDDING_DIM)\n",
        "        self.lstm = torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM)\n",
        "        self.pos_predictor = torch.nn.Linear(HIDDEN_DIM, 44)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        embeds = self.embedding_layer(token_ids)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(token_ids), 1, -1))\n",
        "        logits = self.pos_predictor(lstm_out.view(len(token_ids), -1))\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "        return probs\n",
        "\n",
        "model = LSTMTagger()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "for epoch in range(25):\n",
        "    running_loss = 0\n",
        "    for sentence, tags in training_data:\n",
        "        model.zero_grad()\n",
        "\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        running_loss += loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch}, loss = {running_loss}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    print(training_data[0][0], tag_scores)"
      ]
    }
  ]
}