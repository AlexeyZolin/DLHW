{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "og0Dydlz7yJf",
        "outputId": "781258e8-d71e-4535-b19a-5df1fce87a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0 started==============================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04fa02c94c9a>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_distractor_tens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwork_distractor_tens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "class DistractorsDataset(Dataset):\n",
        "    def __init__(self, dataset_path = 'train.json'):\n",
        "        super().__init__()\n",
        "\n",
        "        with open('train.json') as json_file:\n",
        "          train_data = json.load(json_file)\n",
        "\n",
        "        self.input_list = []\n",
        "        self.end_of_text_token = '<|endoftext|>'\n",
        "\n",
        "        for line in train_data:\n",
        "          dist1 = f\"<question>{line['question']}<key>{line['correct_answer']}<distractor>{line['distractor1']}{self.end_of_text_token}\"\n",
        "          self.input_list.append(dist1)\n",
        "          dist2 = f\"<question>{line['question']}<key>{line['correct_answer']}<distractor>{line['distractor2']}{self.end_of_text_token}\"\n",
        "          self.input_list.append(dist2)\n",
        "          dist3 = f\"<question>{line['question']}<key>{line['correct_answer']}<distractor>{line['distractor3']}{self.end_of_text_token}\"\n",
        "          self.input_list.append(dist3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.input_list[item]\n",
        "\n",
        "dataset = DistractorsDataset()\n",
        "distractors_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_distractor_tens = None\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "    os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "\n",
        "    for idx, distractor in enumerate(distractors_loader):\n",
        "\n",
        "        distractor_tens = torch.tensor(tokenizer.encode(distractor[0])).unsqueeze(0).to(device)\n",
        "        if distractor_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "\n",
        "        if not torch.is_tensor(tmp_distractor_tens):\n",
        "            tmp_distractor_tens = distractor_tens\n",
        "            continue\n",
        "        else:\n",
        "            if tmp_distractor_tens.size()[1] + distractor_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_distractor_tens = tmp_distractor_tens\n",
        "                tmp_distractor_tens = distractor_tens\n",
        "            else:\n",
        "\n",
        "                tmp_distractor_tens = torch.cat([tmp_distractor_tens, distractor_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "\n",
        "\n",
        "        outputs = model(work_distractor_tens, labels=work_distractor_tens)\n",
        "        loss, logits = outputs[:2]\n",
        "        loss.backward()\n",
        "        sum_loss = sum_loss + loss.detach().data\n",
        "\n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0\n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if batch_count == 100:\n",
        "            print(f\"sum loss {sum_loss}\")\n",
        "            batch_count = 0\n",
        "            sum_loss = 0.0\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_distractor_{epoch}.pt\"))\n",
        "\n",
        "MODEL_EPOCH = 4\n",
        "\n",
        "models_folder = \"trained_models\"\n",
        "\n",
        "model_path = os.path.join(models_folder, f\"gpt2_medium_distractor_{MODEL_EPOCH}.pt\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "distractors_output_file_path = f'generated_{MODEL_EPOCH}.distractors'\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if os.path.exists(distractors_output_file_path):\n",
        "    os.remove(distractors_output_file_path)\n",
        "\n",
        "distractor_num = 0\n",
        "\n",
        "questions = ['<question>Who invented radio?<key>Nikola Tesla<distractor>',\n",
        "             '<question>Who invented radio?<key>Nikola Tesla<distractor>',\n",
        "             '<question>Who invented radio?<key>Nikola Tesla<distractor>',\n",
        "             '<question>What force makes the car move?<key>Traction force>',\n",
        "             '<question>What force makes the car move?<key>Traction force>',\n",
        "             '<question>What force makes the car move?<key>Traction force>',\n",
        "             '<Who was the first to receive the Nobel Prize in Physics?<key>Wilhelm Roentgen>',\n",
        "             '<Who was the first to receive the Nobel Prize in Physics?<key>Wilhelm Roentgen>',\n",
        "             '<Who was the first to receive the Nobel Prize in Physics?<key>Wilhelm Roentgen>'\n",
        " ]\n",
        "with torch.no_grad():\n",
        "\n",
        "        for question in questions:\n",
        "\n",
        "            distractor_generation_finished = False\n",
        "\n",
        "            cur_ids = torch.tensor(tokenizer.encode(question)).unsqueeze(0).to(device)\n",
        "\n",
        "            for i in range(100):\n",
        "                outputs = model(cur_ids, labels=cur_ids)\n",
        "                loss, logits = outputs[:2]\n",
        "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
        "                if i < 3:\n",
        "                    n = 20\n",
        "                else:\n",
        "                    n = 3\n",
        "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
        "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
        "\n",
        "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "                    distractor_generation_finished = True\n",
        "                    break\n",
        "\n",
        "\n",
        "            if distractor_generation_finished:\n",
        "\n",
        "                distractor_num = distractor_num + 1\n",
        "\n",
        "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "                output_text = tokenizer.decode(output_list)\n",
        "\n",
        "                with open('generated_distractors.txt', 'a') as f:\n",
        "                    f.write(f\"{output_text} \\n\\n\")"
      ]
    }
  ]
}